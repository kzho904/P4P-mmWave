{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "import os\n",
    "import NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_utils' from 'c:\\\\Users\\\\cyber\\\\Desktop\\\\University\\\\P4P\\\\github\\\\P4P-mmWave\\\\MMWave_Radar_Human_Tracking_and_Fall_detection\\\\dataset_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: jumping\n",
      "processing class: speedwalking\n",
      "processing class: walking\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"3_class_data\"\n",
    "NUM_POINTS = 100\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "train_points, test_points, train_labels, test_labels, CLASS_MAP = dataset_utils.parse_dataset(NUM_POINTS, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 8.540e-01  4.612e-01  1.483e-01  2.505e-01  2.050e+02]\n",
      "  [ 7.290e-01  6.357e-01  4.724e-01  5.010e-01  2.500e+02]\n",
      "  [ 1.000e+00  8.774e-01  7.404e-02  3.757e-01  3.050e+02]\n",
      "  ...\n",
      "  [ 7.437e-01  6.938e-01  2.361e-01  5.010e-01  2.500e+02]\n",
      "  [ 8.892e-01  4.902e-01  1.000e+00  5.010e-01  3.110e+02]\n",
      "  [ 6.753e-01  4.902e-01  4.746e-01  3.757e-01  2.820e+02]]\n",
      "\n",
      " [[ 4.583e-01  2.130e-01  6.841e-01 -7.515e-01  3.500e+02]\n",
      "  [ 1.929e-01  3.770e-01  6.641e-01 -3.757e-01  2.900e+02]\n",
      "  [ 1.929e-01  3.770e-01  6.641e-01 -3.757e-01  2.900e+02]\n",
      "  ...\n",
      "  [ 8.950e-01  5.410e-01  5.840e-01  5.010e-01  3.110e+02]\n",
      "  [ 8.560e-01  2.458e-01  5.532e-01 -5.010e-01  2.940e+02]\n",
      "  [ 8.623e-01  5.103e-01  2.234e-02  2.505e-01  2.050e+02]]\n",
      "\n",
      " [[ 6.709e-01  7.632e-01  5.923e-01 -8.765e-01  2.930e+02]\n",
      "  [ 1.003e-01  0.000e+00  1.000e+00  6.260e-01  3.340e+02]\n",
      "  [ 4.287e-01  3.975e-01  4.424e-01 -8.765e-01  2.570e+02]\n",
      "  ...\n",
      "  [ 1.109e-01  1.000e+00  4.021e-01 -1.002e+00  3.020e+02]\n",
      "  [ 4.595e-01  9.468e-01  3.179e-01 -1.252e-01  1.210e+02]\n",
      "  [ 7.695e-01  2.952e-01  1.000e+00 -7.515e-01  3.240e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.866e-01  6.421e-01  2.375e-01  5.010e-01  3.510e+02]\n",
      "  [ 5.962e-01  0.000e+00  7.910e-01 -3.757e-01  2.520e+02]\n",
      "  [ 5.620e-01  3.140e-01  2.791e-01 -6.260e-01  1.970e+02]\n",
      "  ...\n",
      "  [ 1.000e+00  7.656e-01  0.000e+00  3.757e-01  4.090e+02]\n",
      "  [ 0.000e+00  1.000e+00  7.648e-02  5.010e-01  2.390e+02]\n",
      "  [ 3.987e-01  9.473e-01  1.000e+00  7.515e-01  3.290e+02]]\n",
      "\n",
      " [[ 2.571e-01  2.712e-01  4.482e-01 -3.757e-01  2.890e+02]\n",
      "  [ 6.201e-01  6.011e-01  2.786e-01 -3.757e-01  1.920e+02]\n",
      "  [ 4.607e-01  9.507e-01  7.051e-01  2.505e-01  3.140e+02]\n",
      "  ...\n",
      "  [ 7.026e-01  6.938e-01  7.129e-01 -3.757e-01  2.030e+02]\n",
      "  [ 2.168e-01  3.459e-01  8.525e-01 -3.757e-01  3.090e+02]\n",
      "  [ 7.993e-01  2.399e-01  8.730e-01 -3.757e-01  3.070e+02]]\n",
      "\n",
      " [[ 5.669e-01  1.586e-01  9.961e-01 -6.260e-01  2.500e+02]\n",
      "  [ 2.198e-01  1.351e-01  2.607e-01 -6.260e-01  2.430e+02]\n",
      "  [ 4.939e-01  1.000e+00  2.141e-01 -6.260e-01  3.640e+02]\n",
      "  ...\n",
      "  [ 6.245e-01  8.062e-01  1.656e-01 -5.010e-01  2.890e+02]\n",
      "  [ 3.894e-01  8.271e-01  1.443e-01 -5.010e-01  1.500e+02]\n",
      "  [ 9.297e-01  0.000e+00  2.333e-01 -3.757e-01  1.420e+02]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "#import PointNET\n",
    "\n",
    "keras.utils.set_random_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "dataset = tf_data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf_data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "train_dataset_size = int(len(dataset) * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(len(train_points)).map(NN.augment)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n",
    "train_dataset = dataset.take(train_dataset_size).batch(BATCH_SIZE)\n",
    "validation_dataset = dataset.skip(train_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv3D, MaxPooling3D, Embedding, LSTM, Bidirectional, Reshape, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "# Define the input shape (5 features)\n",
    "input_shape = (100, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, None, 3)           195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1475 (5.76 KB)\n",
      "Trainable params: 1347 (5.26 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 49, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_points, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(test_points, test_labels), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file314iusum.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 49, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# training\n",
    "history = model.fit(train_points, train_labels, epochs=30, batch_size=16, validation_data=(test_points, test_labels), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638, 100, 5)\n",
      "(638,)\n",
      "(162, 100, 5)\n",
      "(162,)\n"
     ]
    }
   ],
   "source": [
    "print(train_points.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_points.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7469 - mae: 0.7469\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_points, test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469135522842407\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_11 (Conv1D)          (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_11 (MaxPooli  (None, None, 64)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, None, 64)          256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, None)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280 (5.00 KB)\n",
      "Trainable params: 1152 (4.50 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input shape (5 features)\n",
    "input_shape = (None, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# Flatten the output\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Summary of the CNN model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming sequence_length = 100 for this example\n",
    "sequence_length = 100\n",
    "#X = np.random.random((10, sequence_length, 5))  # Example data\n",
    "\n",
    "# Extract features using CNN\n",
    "X_Train_Features = model.predict(train_points)\n",
    "X_Test_Features = model.predict(test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 38.27\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_classifer = svm.SVC(kernel='linear')\n",
    "svm_classifer.fit(X_Train_Features, train_labels)\n",
    "\n",
    "y_pred = svm_classifer.predict(X_Test_Features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "162\n",
      "638\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "print(len(X_Train_Features))\n",
    "print(len(X_Test_Features))\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the model is started, please wait for while as it may take few minutes to complete\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "The Model is trained well with the given images\n",
      "The models is 41.358024691358025% accurate\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "svc=svm.SVC(probability=True)\n",
    "print(\"The training of the model is started, please wait for while as it may take few minutes to complete\")\n",
    "model=GridSearchCV(svc,param_grid, refit = True, verbose = 4, n_jobs=-1)\n",
    "model.fit(train_points,train_labels)\n",
    "print('The Model is trained well with the given images')\n",
    "model.best_params_\n",
    "\n",
    "#pickle.dump(model,open('SVM_model.p', 'wb'))\n",
    "y_pred=model.predict(X_Test_Features)\n",
    "\n",
    "print(f\"The models is {accuracy_score(y_pred,test_labels)*100}% accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SVM_model.p', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "y_pred = model.predict(X_Test_Features)\n",
    "accuracy = accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3950617283950617\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = X_Train_Features.reshape(X_Train_Features.shape[0], -1)\n",
    "X_test_flattened = X_Test_Features.reshape(X_Test_Features.shape[0], -1)\n",
    "\n",
    "y_train = train_labels.flatten()\n",
    "y_test = test_labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the model is started, please wait for while as it may take few minutes to complete\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "The Model is trained well with the given images\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not '_BatchDataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe Model is trained well with the given images\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[1;32m---> 11\u001b[0m y_pred\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe models is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy_score(y_pred,test_labels)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m% accurate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:519\u001b[0m, in \u001b[0;36mBaseSearchCV.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Call predict on the estimator with the best found parameters.\u001b[39;00m\n\u001b[0;32m    502\u001b[0m \n\u001b[0;32m    503\u001b[0m \u001b[38;5;124;03mOnly available if ``refit=True`` and the underlying estimator supports\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    516\u001b[0m \u001b[38;5;124;03m    the best found parameters.\u001b[39;00m\n\u001b[0;32m    517\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    518\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:818\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    816\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_function(X), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 818\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[0;32m    819\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\u001b[38;5;241m.\u001b[39mtake(np\u001b[38;5;241m.\u001b[39masarray(y, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mintp))\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:431\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;124;03m        The predicted values.\u001b[39;00m\n\u001b[0;32m    430\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 431\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    432\u001b[0m     predict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_predict \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dense_predict\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:611\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    608\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel):\n\u001b[1;32m--> 611\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    612\u001b[0m         X,\n\u001b[0;32m    613\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    614\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64,\n\u001b[0;32m    615\u001b[0m         order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    616\u001b[0m         accept_large_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    617\u001b[0m         reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39misspmatrix(X):\n\u001b[0;32m    621\u001b[0m     X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:604\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    602\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 604\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    605\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    606\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mTypeError\u001b[0m: float() argument must be a string or a real number, not '_BatchDataset'"
     ]
    }
   ],
   "source": [
    "#pure SVM\n",
    "\n",
    "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "svc=svm.SVC(probability=True)\n",
    "print(\"The training of the model is started, please wait for while as it may take few minutes to complete\")\n",
    "model=GridSearchCV(svc,param_grid, refit = True, verbose = 4, n_jobs=-1)\n",
    "model.fit(train_points,train_labels)\n",
    "print('The Model is trained well with the given images')\n",
    "model.best_params_\n",
    "\n",
    "y_pred=model.predict(test_points)\n",
    "\n",
    "print(f\"The models is {accuracy_score(y_pred,test_labels)*100}% accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 55.56\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_classifier.predict(X_Test_Features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
