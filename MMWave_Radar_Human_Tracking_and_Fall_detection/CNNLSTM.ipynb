{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "import os\n",
    "import NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_utils' from 'c:\\\\Users\\\\cyber\\\\Desktop\\\\University\\\\P4P\\\\github\\\\P4P-mmWave\\\\MMWave_Radar_Human_Tracking_and_Fall_detection\\\\dataset_utils.py'>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: jumping\n",
      "processing class: speedwalking\n",
      "processing class: walking\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"3_class_data\"\n",
    "NUM_POINTS = 100\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "train_points, test_points, train_labels, test_labels, CLASS_MAP = dataset_utils.parse_dataset(NUM_POINTS, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 2.3877e-01  2.5122e-01  5.2765e-02 -7.5146e-01  2.2700e+02]\n",
      "  [ 3.5474e-01  2.5122e-01  1.5710e-01  5.0098e-01  2.0100e+02]\n",
      "  [ 3.5474e-01  2.3584e-01  1.5332e-01  5.0098e-01  2.0100e+02]\n",
      "  ...\n",
      "  [ 7.0654e-01  6.2305e-01  3.2593e-01 -5.0098e-01  2.1300e+02]\n",
      "  [ 0.0000e+00  7.9688e-01  1.0000e+00 -5.0098e-01  3.0600e+02]\n",
      "  [ 5.0635e-01  6.2305e-01  4.8828e-01  2.5049e-01  2.8300e+02]]\n",
      "\n",
      " [[ 6.9141e-01  9.5215e-01  4.6216e-01 -7.5146e-01  2.1200e+02]\n",
      "  [ 0.0000e+00  1.0000e+00  3.1152e-01  3.7573e-01  1.8800e+02]\n",
      "  [ 5.8350e-01  7.7271e-02  7.9980e-01 -5.0098e-01  3.3300e+02]\n",
      "  ...\n",
      "  [ 7.5049e-01  3.9917e-01  7.9834e-01  1.2524e-01  1.4800e+02]\n",
      "  [ 6.2695e-01  5.2393e-01  5.5762e-01 -7.5146e-01  2.9600e+02]\n",
      "  [ 2.8271e-01  3.8257e-01  6.0425e-02  3.7573e-01  2.0200e+02]]\n",
      "\n",
      " [[ 2.8271e-01  3.8257e-01  6.0425e-02  3.7573e-01  2.0200e+02]\n",
      "  [ 5.4102e-01  2.1191e-01  6.6162e-01 -3.7573e-01  2.5300e+02]\n",
      "  [ 5.5029e-01  2.5342e-01  1.0000e+00 -5.0098e-01  2.1100e+02]\n",
      "  ...\n",
      "  [ 1.0000e+00  8.2031e-02  8.2422e-01  3.7573e-01  2.4300e+02]\n",
      "  [ 5.2246e-01  2.8198e-01  1.0000e+00 -5.0098e-01  2.1100e+02]\n",
      "  [ 6.5332e-01  2.3047e-01  7.5830e-01 -8.7646e-01  2.5600e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 5.0977e-01  6.6064e-01  3.6792e-01  3.7573e-01  2.3500e+02]\n",
      "  [ 0.0000e+00  2.3730e-01  4.5947e-01  3.7573e-01  3.4500e+02]\n",
      "  [ 4.1040e-01  0.0000e+00  6.0400e-01  3.7573e-01  3.0000e+02]\n",
      "  ...\n",
      "  [ 0.0000e+00  7.8320e-01  0.0000e+00  6.2598e-01  3.6300e+02]\n",
      "  [ 1.5967e-01  7.3389e-01  1.6504e-01 -6.2598e-01  2.2800e+02]\n",
      "  [ 7.2021e-01  0.0000e+00  2.4817e-01  7.5146e-01  2.2800e+02]]\n",
      "\n",
      " [[ 1.4612e-01  4.3652e-01  6.8066e-01  2.5049e-01  2.8400e+02]\n",
      "  [ 4.8389e-01  2.2839e-01  1.0000e+00 -2.5049e-01  2.8800e+02]\n",
      "  [ 7.8613e-01  1.5283e-01  5.5225e-01 -5.0098e-01  2.2700e+02]\n",
      "  ...\n",
      "  [ 1.0000e+00  5.9033e-01  1.1945e-01 -7.5146e-01  1.8500e+02]\n",
      "  [ 3.8770e-01  6.1816e-01  6.1035e-01 -5.0098e-01  3.2400e+02]\n",
      "  [ 7.3730e-01  3.0737e-01  8.6133e-01 -3.7573e-01  2.0500e+02]]\n",
      "\n",
      " [[ 9.8340e-01  5.6104e-01  3.2837e-01 -6.2598e-01  3.0500e+02]\n",
      "  [ 0.0000e+00  5.4932e-01  1.8103e-01  6.2598e-01  3.3100e+02]\n",
      "  [ 8.1396e-01  2.5342e-01  1.0000e+00  3.7573e-01  3.0000e+02]\n",
      "  ...\n",
      "  [ 2.6489e-01  6.4404e-01  7.5012e-02  7.5146e-01  1.9200e+02]\n",
      "  [ 4.6533e-01  3.1421e-01  5.6738e-01 -6.2598e-01  2.6400e+02]\n",
      "  [ 6.0840e-01  5.7037e-02  8.6670e-02 -2.5049e-01  2.0100e+02]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "#import PointNET\n",
    "\n",
    "keras.utils.set_random_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "dataset = tf_data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf_data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "train_dataset_size = int(len(dataset) * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(len(train_points)).map(NN.augment)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n",
    "train_dataset = dataset.take(train_dataset_size).batch(BATCH_SIZE)\n",
    "validation_dataset = dataset.skip(train_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv3D, MaxPooling3D, Embedding, LSTM, Bidirectional, Reshape, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "# Define the input shape (5 features)\n",
    "input_shape = (None, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# # 2. LSTM layer\n",
    "# model.add(layers.LSTM(50, return_sequences=True))\n",
    "# model.add(layers.LSTM(50))\n",
    "\n",
    "# 3. Dense layers\n",
    "#model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(layers.Dense(1, activation='linear'))  # Output layer (you can adjust the output size and activation)\n",
    "model.add(layers.Dense(1, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 64)          4160      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, None, 1)           65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5505 (21.50 KB)\n",
      "Trainable params: 5377 (21.00 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "15/15 [==============================] - 2s 24ms/step - loss: 0.6667 - mae: 0.6667 - val_loss: 0.6667 - val_mae: 0.6667\n",
      "Epoch 2/30\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6667 - mae: 0.6667 - val_loss: 0.6667 - val_mae: 0.6667\n",
      "Epoch 3/30\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6667 - mae: 0.6667 - val_loss: 0.6667 - val_mae: 0.6667\n",
      "Epoch 4/30\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.6667 - mae: 0.6667 - val_loss: 0.6667 - val_mae: 0.6667\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# training\n",
    "history = model.fit(train_points, train_labels, epochs=30, batch_size=16, validation_data=(test_points, test_labels), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6667 - mae: 0.6667\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_points, test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666668057441711\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_8 (Conv1D)           (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_8 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_9 (Conv1D)           (None, None, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          (None, None, 64)          12352     \n",
      "                                                                 \n",
      " max_pooling1d_10 (MaxPooli  (None, None, 64)          0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, None, 64)          256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, None)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 26496 (103.50 KB)\n",
      "Trainable params: 26112 (102.00 KB)\n",
      "Non-trainable params: 384 (1.50 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input shape (5 features)\n",
    "input_shape = (None, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Flatten the output\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Summary of the CNN model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming sequence_length = 100 for this example\n",
    "sequence_length = 100\n",
    "#X = np.random.random((10, sequence_length, 5))  # Example data\n",
    "\n",
    "# Extract features using CNN\n",
    "X_Train_Features = model.predict(train_points)\n",
    "X_Test_Features = model.predict(test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 51.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_classifer = svm.SVC(kernel='linear')\n",
    "svm_classifer.fit(X_Train_Features, train_labels)\n",
    "\n",
    "y_pred = svm_classifer.predict(X_Test_Features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "60\n",
      "240\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(X_Train_Features))\n",
    "print(len(X_Test_Features))\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the model is started, please wait for while as it may take few minutes to complete\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "The Model is trained well with the given images\n",
      "The models is 56.666666666666664% accurate\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "svc=svm.SVC(probability=True)\n",
    "print(\"The training of the model is started, please wait for while as it may take few minutes to complete\")\n",
    "model=GridSearchCV(svc,param_grid, refit = True, verbose = 4, n_jobs=-1)\n",
    "model.fit(X_Train_Features,train_labels)\n",
    "print('The Model is trained well with the given images')\n",
    "model.best_params_\n",
    "\n",
    "pickle.dump(model,open('SVM_model.p', 'wb'))\n",
    "y_pred=model.predict(X_Test_Features)\n",
    "\n",
    "print(f\"The models is {accuracy_score(y_pred,test_labels)*100}% accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SVM_model.p', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "y_pred = model.predict(X_Test_Features)\n",
    "accuracy = accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5666666666666667\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
