{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "import os\n",
    "import NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataset_utils\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'dataset_utils' from 'c:\\\\Users\\\\cyber\\\\Desktop\\\\University\\\\P4P\\\\github\\\\P4P-mmWave\\\\MMWave_Radar_Human_Tracking_and_Fall_detection\\\\dataset_utils.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(dataset_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: jumping\n",
      "processing class: speedwalking\n",
      "processing class: walking\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"3_class_data\"\n",
    "NUM_POINTS = 100\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "train_points, test_points, train_labels, test_labels, CLASS_MAP = dataset_utils.parse_dataset(NUM_POINTS, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 3.042e-01  0.000e+00  3.391e-02  7.515e-01  1.530e+02]\n",
      "  [ 8.442e-01  5.000e-01  5.181e-01 -3.757e-01  2.740e+02]\n",
      "  [ 1.000e+00  1.000e+00  4.783e-01  3.757e-01  3.050e+02]\n",
      "  ...\n",
      "  [ 3.042e-01  0.000e+00  3.391e-02  7.515e-01  1.530e+02]\n",
      "  [ 8.950e-01  0.000e+00  1.000e+00  5.010e-01  2.150e+02]\n",
      "  [ 1.000e+00  8.774e-01  7.404e-02  3.757e-01  3.050e+02]]\n",
      "\n",
      " [[ 8.560e-01  2.458e-01  5.532e-01 -5.010e-01  2.940e+02]\n",
      "  [ 7.578e-01  7.378e-01  5.635e-01  2.505e-01  3.070e+02]\n",
      "  [ 5.664e-01  7.817e-01  4.617e-01  6.260e-01  3.230e+02]\n",
      "  ...\n",
      "  [ 4.026e-01  8.057e-01  3.135e-01  1.252e-01  1.170e+02]\n",
      "  [ 5.166e-01  9.370e-01  4.407e-01  8.765e-01  2.750e+02]\n",
      "  [ 8.623e-01  5.103e-01  2.234e-02  2.505e-01  2.050e+02]]\n",
      "\n",
      " [[ 6.455e-01  8.271e-01  3.313e-01  5.010e-01  3.270e+02]\n",
      "  [ 4.746e-01  4.783e-01  7.046e-01  0.000e+00  2.600e+02]\n",
      "  [ 1.000e+00  5.693e-01  4.822e-01 -8.765e-01  2.570e+02]\n",
      "  ...\n",
      "  [ 8.540e-01  4.612e-01  1.483e-01  2.505e-01  2.050e+02]\n",
      "  [ 4.514e-01  9.171e-03  9.951e-01 -5.010e-01  3.800e+02]\n",
      "  [ 4.929e-01  3.262e-01  5.132e-01  8.765e-01  2.510e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0.000e+00  3.977e-01  4.636e-01 -3.757e-01  3.070e+02]\n",
      "  [ 6.943e-01  7.964e-01  1.000e+00 -7.515e-01  2.470e+02]\n",
      "  [ 2.546e-01  4.099e-02  3.127e-01 -7.515e-01  3.000e+02]\n",
      "  ...\n",
      "  [ 4.641e-01  6.870e-01  1.698e-01  5.010e-01  1.910e+02]\n",
      "  [ 9.883e-01  1.371e-01  8.506e-01 -3.757e-01  2.380e+02]\n",
      "  [ 3.684e-01  4.067e-01  2.534e-01  0.000e+00  1.820e+02]]\n",
      "\n",
      " [[ 7.861e-01  0.000e+00  0.000e+00  5.010e-01  3.290e+02]\n",
      "  [ 1.383e-01  4.856e-01  4.836e-01  2.505e-01  3.770e+02]\n",
      "  [ 9.546e-02  0.000e+00  1.000e+00  1.252e-01  1.870e+02]\n",
      "  ...\n",
      "  [ 1.174e-01  6.919e-01  5.391e-01 -5.010e-01  3.010e+02]\n",
      "  [ 5.234e-01  8.130e-01  6.685e-01  5.010e-01  2.510e+02]\n",
      "  [ 1.000e+00  2.274e-01  8.130e-01 -7.515e-01  3.210e+02]]\n",
      "\n",
      " [[ 6.133e-01  5.918e-01  8.726e-01 -3.757e-01  3.600e+02]\n",
      "  [ 8.356e-02  2.325e-01  3.013e-01 -8.765e-01  2.500e+02]\n",
      "  [ 7.085e-01  3.467e-01  3.577e-01 -2.505e-01  2.100e+02]\n",
      "  ...\n",
      "  [ 6.929e-01  7.612e-01  8.687e-01  6.260e-01  2.360e+02]\n",
      "  [ 9.678e-01  4.321e-01  6.860e-01  2.505e-01  3.900e+02]\n",
      "  [ 6.548e-01  3.369e-01  6.626e-01 -1.252e-01  3.070e+02]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "#import PointNET\n",
    "\n",
    "keras.utils.set_random_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.8\n",
    "dataset = tf_data.Dataset.from_tensor_slices((train_points, train_labels))\n",
    "test_dataset = tf_data.Dataset.from_tensor_slices((test_points, test_labels))\n",
    "train_dataset_size = int(len(dataset) * train_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(len(train_points)).map(NN.augment)\n",
    "test_dataset = test_dataset.shuffle(len(test_points)).batch(BATCH_SIZE)\n",
    "train_dataset = dataset.take(train_dataset_size).batch(BATCH_SIZE)\n",
    "validation_dataset = dataset.skip(train_dataset_size).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(NUM_POINTS, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv3D, MaxPooling3D, Embedding, LSTM, Bidirectional, Reshape, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "# Define the input shape (5 features)\n",
    "input_shape = (None, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# # 2. LSTM layer\n",
    "# model.add(layers.LSTM(50, return_sequences=True))\n",
    "# model.add(layers.LSTM(50))\n",
    "\n",
    "# 3. Dense layers\n",
    "#model.add(layers.Flatten())\n",
    "#model.add(layers.Dense(64, activation='relu'))\n",
    "#model.add(BatchNormalization())\n",
    "#model.add(Dropout(0.5))\n",
    "#model.add(layers.Dense(1, activation='linear'))  # Output layer (you can adjust the output size and activation)\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_9 (Conv1D)           (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_9 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, None, 3)           195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1475 (5.76 KB)\n",
      "Trainable params: 1347 (5.26 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 49, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[53], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(train_points, train_labels, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(test_points, test_labels), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])\n",
      "File \u001b[1;32mc:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file314iusum.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1081, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\training.py\", line 1139, in compute_loss\n        return self.compiled_loss(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py\", line 2122, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"c:\\Users\\cyber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py\", line 5560, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None,) and (None, 49, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# training\n",
    "history = model.fit(train_points, train_labels, epochs=30, batch_size=16, validation_data=(test_points, test_labels), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638, 100, 5)\n",
      "(638,)\n",
      "(162, 100, 5)\n",
      "(162,)\n"
     ]
    }
   ],
   "source": [
    "print(train_points.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_points.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 2ms/step - loss: 0.7469 - mae: 0.7469\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_points, test_labels, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7469135522842407\n"
     ]
    }
   ],
   "source": [
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(model, open(\"model.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_4 (Conv1D)           (None, None, 64)          1024      \n",
      "                                                                 \n",
      " max_pooling1d_4 (MaxPoolin  (None, None, 64)          0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, None, 64)          0         \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, None, 64)          256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, None)              0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1280 (5.00 KB)\n",
      "Trainable params: 1152 (4.50 KB)\n",
      "Non-trainable params: 128 (512.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the input shape (5 features)\n",
    "input_shape = (None, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layer\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "#model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "#model.add(layers.MaxPooling1D(pool_size=2))\n",
    "#model.add(Dropout(0.25))\n",
    "#model.add(BatchNormalization())\n",
    "\n",
    "# Flatten the output\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# Summary of the CNN model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 2ms/step\n",
      "6/6 [==============================] - 0s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming sequence_length = 100 for this example\n",
    "sequence_length = 100\n",
    "#X = np.random.random((10, sequence_length, 5))  # Example data\n",
    "\n",
    "# Extract features using CNN\n",
    "X_Train_Features = model.predict(train_points)\n",
    "X_Test_Features = model.predict(test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 39.51\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "svm_classifer = svm.SVC(kernel='linear')\n",
    "svm_classifer.fit(X_Train_Features, train_labels)\n",
    "\n",
    "y_pred = svm_classifer.predict(X_Test_Features)\n",
    "\n",
    "accuracy = accuracy_score(test_labels, y_pred)\n",
    "print('Accuracy: %.2f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "638\n",
      "162\n",
      "638\n",
      "162\n"
     ]
    }
   ],
   "source": [
    "print(len(X_Train_Features))\n",
    "print(len(X_Test_Features))\n",
    "print(len(train_labels))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The training of the model is started, please wait for while as it may take few minutes to complete\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n",
      "The Model is trained well with the given images\n",
      "The models is 41.358024691358025% accurate\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "svc=svm.SVC(probability=True)\n",
    "print(\"The training of the model is started, please wait for while as it may take few minutes to complete\")\n",
    "model=GridSearchCV(svc,param_grid, refit = True, verbose = 4, n_jobs=-1)\n",
    "model.fit(X_Train_Features,train_labels)\n",
    "print('The Model is trained well with the given images')\n",
    "model.best_params_\n",
    "\n",
    "pickle.dump(model,open('SVM_model.p', 'wb'))\n",
    "y_pred=model.predict(X_Test_Features)\n",
    "\n",
    "print(f\"The models is {accuracy_score(y_pred,test_labels)*100}% accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SVM_model.p', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "y_pred = model.predict(X_Test_Features)\n",
    "accuracy = accuracy_score(test_labels, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3950617283950617\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
