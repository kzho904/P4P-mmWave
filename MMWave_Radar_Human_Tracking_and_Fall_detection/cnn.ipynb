{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D\n",
    "from keras.utils import to_categorical\n",
    "import pickle\n",
    "import os\n",
    "import NN\n",
    "\n",
    "import dataset_utils\n",
    "import importlib\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import LinearLocator\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Conv3D, MaxPooling3D, Embedding, LSTM, Bidirectional, Reshape, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing class: jumping\n",
      "processing class: speedwalking\n",
      "processing class: walking\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"3_class_data\"\n",
    "NUM_POINTS = 100\n",
    "NUM_CLASSES = 2\n",
    "BATCH_SIZE = 32\n",
    "train_points, test_points, train_labels, test_labels, CLASS_MAP = dataset_utils.parse_dataset(NUM_POINTS, DATA_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 7.437e-01  6.938e-01  2.361e-01  5.010e-01  2.500e+02]\n",
      "  [ 3.042e-01  0.000e+00  3.391e-02  7.515e-01  1.530e+02]\n",
      "  [ 1.003e-01  0.000e+00  1.000e+00  6.260e-01  3.340e+02]\n",
      "  ...\n",
      "  [ 1.000e+00  8.105e-01  2.267e-01  3.757e-01  3.050e+02]\n",
      "  [ 8.950e-01  3.061e-02  9.751e-01 -2.505e-01  2.630e+02]\n",
      "  [ 8.442e-01  5.000e-01  5.181e-01 -3.757e-01  2.740e+02]]\n",
      "\n",
      " [[ 6.929e-01  5.410e-01  2.374e-01  3.757e-01  2.820e+02]\n",
      "  [ 0.000e+00  4.932e-01  0.000e+00  8.765e-01  3.000e+02]\n",
      "  [ 0.000e+00  4.058e-01  4.167e-01 -5.010e-01  3.220e+02]\n",
      "  ...\n",
      "  [ 6.997e-01  6.001e-01  7.402e-01 -8.765e-01  2.780e+02]\n",
      "  [ 1.000e+00  7.271e-01  2.203e-02 -6.260e-01  1.720e+02]\n",
      "  [ 5.723e-01  8.076e-01  3.647e-01 -1.002e+00  2.630e+02]]\n",
      "\n",
      " [[ 0.000e+00  3.384e-01  3.237e-01 -2.505e-01  3.210e+02]\n",
      "  [ 7.144e-01  8.481e-01  5.039e-01  1.252e-01  2.190e+02]\n",
      "  [ 5.371e-01  6.885e-01  6.777e-01 -1.252e-01  2.510e+02]\n",
      "  ...\n",
      "  [ 1.715e-01  5.728e-01  7.139e-01 -5.010e-01  3.500e+02]\n",
      "  [ 4.841e-01  2.037e-01  7.124e-01 -5.010e-01  3.150e+02]\n",
      "  [ 7.578e-01  5.894e-01  6.255e-01 -8.765e-01  3.090e+02]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 1.184e-01  8.726e-01  1.907e-01  6.260e-01  2.470e+02]\n",
      "  [ 6.206e-01  2.886e-01  6.860e-01 -5.010e-01  3.250e+02]\n",
      "  [ 4.902e-01  6.099e-01  2.783e-01  2.505e-01  2.310e+02]\n",
      "  ...\n",
      "  [ 1.610e-01  6.914e-01  4.009e-01 -7.515e-01  3.280e+02]\n",
      "  [ 2.546e-01  4.043e-01  1.428e-01  3.757e-01  3.500e+02]\n",
      "  [ 5.894e-01  5.532e-01  2.458e-01 -8.765e-01  2.090e+02]]\n",
      "\n",
      " [[ 6.689e-02  7.930e-01  7.349e-02  7.515e-01  2.780e+02]\n",
      "  [ 1.879e-01  6.499e-01  3.418e-01 -7.515e-01  2.210e+02]\n",
      "  [ 1.652e-01  9.956e-01  6.152e-01 -3.757e-01  2.940e+02]\n",
      "  ...\n",
      "  [ 4.905e-01  4.668e-01  7.534e-01 -6.260e-01  3.020e+02]\n",
      "  [ 2.354e-01  5.791e-01  1.466e-01 -8.765e-01  2.260e+02]\n",
      "  [ 8.511e-01  0.000e+00  3.318e-01  2.505e-01  2.110e+02]]\n",
      "\n",
      " [[ 6.851e-01  8.687e-01  2.759e-01 -5.010e-01  2.740e+02]\n",
      "  [ 7.163e-01  0.000e+00  6.976e-02 -8.765e-01  2.240e+02]\n",
      "  [ 3.420e-01  6.206e-01  2.015e-01  0.000e+00  2.240e+02]\n",
      "  ...\n",
      "  [ 3.474e-01  5.342e-01  2.123e-01  3.757e-01  3.960e+02]\n",
      "  [ 3.848e-01  7.388e-01  6.841e-01 -6.260e-01  2.470e+02]\n",
      "  [ 4.365e-01  4.697e-01  9.834e-01  5.010e-01  2.600e+02]]]\n"
     ]
    }
   ],
   "source": [
    "print(train_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(638, 100, 5)\n",
      "(162, 100, 5)\n",
      "(638,)\n",
      "(162,)\n"
     ]
    }
   ],
   "source": [
    "print(train_points.shape)\n",
    "print(test_points.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import data as tf_data\n",
    "import keras\n",
    "#import PointNET\n",
    "\n",
    "keras.utils.set_random_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, BatchNormalization\n",
    "\n",
    "# Define the input shape (5 features)\n",
    "sequence_length = 100  # Example: Fixed sequence length of 100 timesteps\n",
    "\n",
    "input_shape = (sequence_length, 5)\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential()\n",
    "\n",
    "# 1. CNN layers\n",
    "model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "model.add(layers.MaxPooling1D(pool_size=2))\n",
    "# model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "# model.add(layers.MaxPooling1D(pool_size=2))\n",
    "\n",
    "# 2. Flatten the output before feeding into Dense layers\n",
    "model.add(layers.Flatten())\n",
    "\n",
    "# 3. Fully connected layer\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1d_20 (Conv1D)          (None, 98, 64)            1024      \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " max_pooling1d_20 (MaxPooli  (None, 49, 64)            0         \n",
      " ng1D)                                                           \n",
      "                                                                 \n",
      " flatten_12 (Flatten)        (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 64)                200768    \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 3)                 195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 201987 (789.01 KB)\n",
      "Trainable params: 201987 (789.01 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 1s 40ms/step - loss: 1.1763 - accuracy: 0.6301 - val_loss: 3.6920 - val_accuracy: 0.2963\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.9728 - accuracy: 0.6912 - val_loss: 4.7199 - val_accuracy: 0.3272\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 0s 10ms/step - loss: 0.7617 - accuracy: 0.7335 - val_loss: 3.5104 - val_accuracy: 0.3210\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.5790 - accuracy: 0.7712 - val_loss: 4.2853 - val_accuracy: 0.3086\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.5744 - accuracy: 0.7884 - val_loss: 3.4457 - val_accuracy: 0.3086\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 0s 7ms/step - loss: 0.3839 - accuracy: 0.8480 - val_loss: 3.4716 - val_accuracy: 0.3025\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.5108 - accuracy: 0.7727 - val_loss: 3.7508 - val_accuracy: 0.2963\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 0s 8ms/step - loss: 0.6251 - accuracy: 0.7696 - val_loss: 4.0955 - val_accuracy: 0.3086\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "# training\n",
    "history = model.fit(train_points, train_labels, epochs=30, batch_size=32, validation_data=(test_points, test_labels), verbose=1, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 0s 6ms/step - loss: 1.6617 - accuracy: 0.3148\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_points, test_labels, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
